#!/usr/bin/env python3
"""
USMS Masters NQT scraper ‚Üí Supabase standards_usa

v5.1  (SCY + LCM)  - robust fetch (browser-like headers + Playwright fallback)
Why:
  USMS often blocks python-requests with 403 in CI (GitHub Actions).
  This script:
    1) tries requests with realistic browser headers
    2) if blocked (403/429) tries Playwright Chromium to fetch the rendered HTML
    3) optional PDF fallback if URLs are provided (env PDF_URL_SCY / PDF_URL_LCM)

Destination: standards_usa

Env:
  SUPABASE_URL, SUPABASE_KEY
  (optional) MAIL_USERNAME/EMAIL_USER, MAIL_PASSWORD/EMAIL_PASSWORD
  (optional) USMS_NQT_URL
  (optional) PDF_URL_SCY, PDF_URL_LCM
  (optional) FETCH_MODE = auto|requests|playwright (default auto)
"""
import os
import re
import io
import datetime
import requests
import pdfplumber
import smtplib
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple

from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from dotenv import load_dotenv
from supabase import create_client
from bs4 import BeautifulSoup

# ----------------------------
# Config
# ----------------------------
load_dotenv()

SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
if not SUPABASE_URL or not SUPABASE_KEY:
    raise SystemExit("Missing SUPABASE_URL or SUPABASE_KEY env vars")

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

EMAIL_SENDER = os.environ.get("MAIL_USERNAME") or os.environ.get("EMAIL_USER")
EMAIL_PASSWORD = os.environ.get("MAIL_PASSWORD") or os.environ.get("EMAIL_PASSWORD")
EMAIL_RECEIVER = os.environ.get("EMAIL_RECEIVER") or "vorrabermauro@gmail.com"

USMS_NQT_URL = os.environ.get(
    "USMS_NQT_URL",
    "https://www.usms.org/events/national-championships/pool-national-championships/national-qualifying-times",
)

DEFAULT_PDF_URL_SCY = "https://www-usms-hhgdctfafngha6hr.z01.azurefd.net/-/media/usms/pdfs/pool%20national%20championships/2025%20spring%20nationals/2025%20usms%20spring%20nationals%20nqts%20v2.pdf"
PDF_URL_SCY = os.environ.get("PDF_URL_SCY") or DEFAULT_PDF_URL_SCY
PDF_URL_LCM = os.environ.get("PDF_URL_LCM") or ""

FETCH_MODE = (os.environ.get("FETCH_MODE") or "auto").strip().lower()  # auto|requests|playwright

# Stats for email/report
STATS = {
    "extracted": 0,
    "inserted": 0,
    "male": 0,
    "female": 0,
    "errors": 0,
    "courses": {},
    "seasons": {},
}

REQUEST_HEADERS = {
    # Avoid python-requests UA blocks
    "User-Agent": (
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9,es-AR;q=0.8,es;q=0.7",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
}

# ----------------------------
# Email report
# ----------------------------
def enviar_reporte_email(log_body: str, status: str = "SUCCESS") -> None:
    if not EMAIL_SENDER or not EMAIL_PASSWORD:
        print("‚ö†Ô∏è No email credentials found. Skipping email.")
        return
    try:
        msg = MIMEMultipart()
        msg["From"] = f"Bot MDV <{EMAIL_SENDER}>"
        msg["To"] = EMAIL_RECEIVER
        icon = "üü¢" if status == "SUCCESS" else "üî¥"
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        msg["Subject"] = f"{icon} Reporte USMS Masters NQT - {timestamp}"

        msg.attach(MIMEText(log_body, "plain"))

        server = smtplib.SMTP("smtp.gmail.com", 587)
        server.starttls()
        server.login(EMAIL_SENDER, EMAIL_PASSWORD)
        server.sendmail(EMAIL_SENDER, EMAIL_RECEIVER, msg.as_string())
        server.quit()
        print("üìß Email de reporte enviado correctamente.")
    except Exception as e:
        print(f"‚ùå Error enviando email: {e}")

# ----------------------------
# Helpers: normalization
# ----------------------------
def clean_time_to_seconds(s: str) -> Optional[float]:
    if s is None:
        return None
    s = str(s).strip().replace("*", "").replace("+", "")
    if not s:
        return None
    try:
        if ":" in s:
            parts = s.split(":")
            if len(parts) == 2:
                return float(parts[0]) * 60 + float(parts[1])
            if len(parts) == 3:
                return float(parts[0]) * 3600 + float(parts[1]) * 60 + float(parts[2])
        return float(s)
    except Exception:
        return None

STROKE_MAP = {
    "free": "Libre",
    "freestyle": "Libre",
    "back": "Espalda",
    "backstroke": "Espalda",
    "breast": "Pecho",
    "breaststroke": "Pecho",
    "fly": "Mariposa",
    "butterfly": "Mariposa",
    "im": "Combinado",
    "individual medley": "Combinado",
}

def norm_stroke(txt: str) -> Optional[str]:
    if not txt:
        return None
    t = re.sub(r"\s+", " ", txt.strip().lower())
    for k, v in STROKE_MAP.items():
        if k in t:
            return v
    return None

def parse_event_label(label: str) -> Tuple[Optional[int], Optional[str]]:
    # label examples: "50 Freestyle", "100 Backstroke", "200 IM"
    if not label:
        return (None, None)
    m = re.search(r"(\d+)", label)
    dist = int(m.group(1)) if m else None
    stroke = norm_stroke(label)
    return dist, stroke

def parse_age_band(txt: str) -> Optional[str]:
    if not txt:
        return None
    t = txt.strip()
    # Examples: "18-24" / "95+" / "120-159" / "120+"
    m = re.search(r"(\d{2,3}\s*\+\s*|\d{2,3}\s*-\s*\d{2,3}|\d{2,3}\+)", t)
    if not m:
        return None
    s = m.group(1).replace(" ", "")
    s = s.replace("95+", "95+")
    s = s.replace("120+", "120+")
    return s

@dataclass
class SectionInfo:
    season_year: str
    course: str  # SCY/LCM
    gender: str  # M/F

def iter_sections_with_tables(html: str) -> List[Tuple[SectionInfo, str]]:
    """
    Finds NQT tables on the page and associates each with:
      - course (SCY vs LCM)
      - gender
      - season_year (best-effort; defaults to current year)
    The USMS page structure can change; this uses flexible heuristics.
    """
    soup = BeautifulSoup(html, "lxml")

    # USMS page typically has multiple headings and tables; we walk and capture nearest heading text.
    tables = soup.find_all("table")
    results: List[Tuple[SectionInfo, str]] = []

    current_year = str(datetime.datetime.now().year)

    for tbl in tables:
        # Collect context text from previous headings
        ctx = []
        prev = tbl
        for _ in range(8):
            prev = prev.find_previous(["h2", "h3", "h4", "p", "strong", "div"])
            if not prev:
                break
            txt = " ".join(prev.get_text(" ", strip=True).split())
            if txt:
                ctx.append(txt)
            # stop early if we already found something meaningful
            if any(k in txt.lower() for k in ["spring", "summer", "scy", "lcm", "women", "men"]):
                break

        ctx_text = " | ".join(ctx).lower()

        course = "SCY" if ("scy" in ctx_text or "spring" in ctx_text) else ("LCM" if ("lcm" in ctx_text or "summer" in ctx_text) else "")
        gender = "F" if ("women" in ctx_text or "female" in ctx_text) else ("M" if ("men" in ctx_text or "male" in ctx_text) else "")

        # attempt extract year
        ym = re.search(r"(20\d{2})", ctx_text)
        season_year = ym.group(1) if ym else current_year

        if not course or not gender:
            # skip unrelated tables
            continue

        info = SectionInfo(season_year=season_year, course=course, gender=gender)
        results.append((info, str(tbl)))

    return results

def parse_table_to_rows(info: SectionInfo, table_html: str) -> List[Dict]:
    """
    Parses one NQT table:
      - first column: event label (distance + stroke)
      - subsequent columns: age bands (18-24, 25-29, ..., 95+ / 120+ etc)
    """
    soup = BeautifulSoup(table_html, "lxml")
    table = soup.find("table")
    if not table:
        return []

    # header
    thead = table.find("thead")
    header_cells = []
    if thead:
        header_cells = [c.get_text(" ", strip=True) for c in thead.find_all(["th", "td"])]
    if not header_cells:
        first_row = table.find("tr")
        if first_row:
            header_cells = [c.get_text(" ", strip=True) for c in first_row.find_all(["th", "td"])]

    # Determine age columns
    age_cols: List[Tuple[int, str]] = []  # (col_index, age_band)
    for i, h in enumerate(header_cells):
        age = parse_age_band(h)
        if age:
            age_cols.append((i, age))

    if not age_cols:
        return []

    rows_out: List[Dict] = []
    body_rows = table.find_all("tr")
    # Skip header row if it looks like one
    for r in body_rows[1:]:
        cells = [c.get_text(" ", strip=True) for c in r.find_all(["td", "th"])]
        if not cells or len(cells) < (max(i for i,_ in age_cols) + 1):
            continue

        event_label = cells[0]
        dist, stroke = parse_event_label(event_label)
        if not dist or not stroke:
            continue

        for col_idx, age in age_cols:
            t = cells[col_idx] if col_idx < len(cells) else ""
            secs = clean_time_to_seconds(t)
            if secs is None:
                continue

            rows_out.append({
                "ciclo": info.season_year,
                "season_year": info.season_year,
                "standard_type": "MASTERS",
                "nivel": "NQT",
                "genero": info.gender,
                "edad": age,
                "estilo": stroke,
                "distancia_m": dist,
                "curso": info.course,
                "tiempo_s": secs,
            })

    return rows_out

# ----------------------------
# Robust HTML fetch
# ----------------------------
def fetch_html_requests(url: str) -> str:
    sess = requests.Session()
    # warmup to get cookies
    try:
        sess.get("https://www.usms.org/", headers=REQUEST_HEADERS, timeout=30)
    except Exception:
        pass

    r = sess.get(url, headers=REQUEST_HEADERS, timeout=30, allow_redirects=True)
    # Explicit block detection
    if r.status_code in (403, 429):
        raise RuntimeError(f"{r.status_code} {r.reason}")
    r.raise_for_status()
    # sometimes 200 but contains block page
    low = r.text.lower()
    if "access denied" in low or "forbidden" in low and "cloudflare" in low:
        raise RuntimeError("Blocked content (access denied)")
    return r.text

def fetch_html_playwright(url: str) -> str:
    try:
        from playwright.sync_api import sync_playwright
    except Exception as e:
        raise RuntimeError(f"Playwright not available: {e}")

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(
            user_agent=REQUEST_HEADERS["User-Agent"],
            locale="en-US",
            extra_http_headers={
                "Accept-Language": REQUEST_HEADERS["Accept-Language"],
            },
        )
        page = context.new_page()
        page.goto(url, wait_until="networkidle", timeout=90_000)
        html = page.content()
        browser.close()
        if not html or len(html) < 1000:
            raise RuntimeError("Playwright fetched empty/too-small HTML")
        return html

def fetch_html_auto(url: str) -> str:
    if FETCH_MODE == "requests":
        return fetch_html_requests(url)
    if FETCH_MODE == "playwright":
        return fetch_html_playwright(url)

    # auto
    try:
        return fetch_html_requests(url)
    except Exception as e:
        print(f"‚ö†Ô∏è  requests fetch blocked/failed: {e}. Trying Playwright‚Ä¶")
        return fetch_html_playwright(url)

# ----------------------------
# Supabase upsert strategy
# ----------------------------
def delete_existing_for(season_year: str, curso: str) -> None:
    # Only wipe the slice we are refreshing
    supabase.table("standards_usa")\
        .delete()\
        .eq("standard_type", "MASTERS")\
        .eq("nivel", "NQT")\
        .eq("season_year", season_year)\
        .eq("curso", curso)\
        .execute()

def insert_batches(rows: List[Dict], batch_size: int = 500) -> None:
    total = len(rows)
    for i in range(0, total, batch_size):
        batch = rows[i:i+batch_size]
        supabase.table("standards_usa").insert(batch).execute()
        print(f"   üíâ Insert lote {i} a {min(i+batch_size, total)}")

# ----------------------------
# Optional PDF fallback (kept minimal; you can plug your existing PDF parser if needed)
# ----------------------------
def extraer_tiempo_testigo(table) -> float:
    """
    Heur√≠stica para asignar g√©nero a dos tablas (Men/Women) dentro de una p√°gina:
    busca 50 Free 18-24 y compara el tiempo.
    """
    try:
        header_idx = -1
        for i, row in enumerate(table):
            row_str = " ".join([str(c) for c in row if c])
            if "18-24" in row_str:
                header_idx = i
                break
        if header_idx == -1:
            return 9999.0

        for row in table[header_idx + 1 :]:
            row_clean = [str(c).replace("\n", " ").strip().upper() for c in row if c]
            if not row_clean:
                continue
            evt = row_clean[0]
            if "50" in evt and "FREE" in evt:
                time_val = row[1]
                t_seg = clean_time_to_seconds(time_val)
                if t_seg:
                    return float(t_seg)
    except Exception:
        pass
    return 9999.0


def procesar_tablas_pdf(pdf_bytes: bytes, curso: str, season_year: str) -> List[Dict]:
    """
    PDF fallback (pdfplumber tables).
    Nota: este parser est√° probado para PDFs tipo "Spring Nationals NQTs".
    Si USMS cambia el layout, puede requerir ajustes.
    """
    data_to_insert: List[Dict] = []
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
        print(f"   üìÑ Analizando PDF ({curso})...")
        for page in pdf.pages:
            tables = page.extract_tables()
            if not tables:
                continue

            mapa_generos = {}
            if len(tables) >= 2:
                t0 = extraer_tiempo_testigo(tables[0])
                t1 = extraer_tiempo_testigo(tables[1])
                # menor tiempo suele ser Men
                if t0 < t1 and t0 > 0:
                    mapa_generos[0], mapa_generos[1] = "M", "F"
                elif t1 < t0 and t1 > 0:
                    mapa_generos[0], mapa_generos[1] = "F", "M"
                else:
                    mapa_generos[0], mapa_generos[1] = "F", "M"
            else:
                page_text = (page.extract_text() or "").upper()
                if "WOMEN" in page_text and "MEN" not in page_text:
                    mapa_generos[0] = "F"
                elif "MEN" in page_text and "WOMEN" not in page_text:
                    mapa_generos[0] = "M"
                else:
                    mapa_generos[0] = "F"

            for i, table in enumerate(tables):
                genero = mapa_generos.get(i, "X")

                header_idx = -1
                age_groups = []
                for idx_row, row in enumerate(table):
                    row_str = " ".join([str(c) for c in row if c])
                    if "18-24" in row_str:
                        header_idx = idx_row
                        age_groups = row
                        break
                if header_idx == -1:
                    continue

                for row in table[header_idx + 1 :]:
                    row = [col if col else "" for col in row]
                    if len(row) < 2:
                        continue

                    event_name = str(row[0]).replace("\n", " ").strip()
                    if not event_name or "RELAY" in event_name.upper():
                        continue

                    parts = event_name.split()
                    if not parts or not parts[0].isdigit():
                        continue
                    distancia = int(parts[0])
                    estilo_raw = " ".join(parts[1:]).upper()

                    estilo = None
                    if "FREE" in estilo_raw:
                        estilo = "Libre"
                    elif "BACK" in estilo_raw:
                        estilo = "Espalda"
                    elif "BREAST" in estilo_raw:
                        estilo = "Pecho"
                    elif "FLY" in estilo_raw:
                        estilo = "Mariposa"
                    elif "IM" in estilo_raw:
                        estilo = "Combinado"
                    if not estilo:
                        continue

                    for col_idx, time_val in enumerate(row):
                        if col_idx == 0:
                            continue
                        if col_idx >= len(age_groups):
                            break

                        age_range = str(age_groups[col_idx]).replace("\n", "").strip()
                        if not age_range:
                            continue
                        if "NO TIME" in str(time_val).upper():
                            continue

                        t_seg = clean_time_to_seconds(time_val)
                        if t_seg is None:
                            continue

                        data_to_insert.append(
                            {
                                "ciclo": season_year,
                                "season_year": season_year,
                                "standard_type": "MASTERS",
                                "nivel": "NQT",
                                "genero": genero,
                                "edad": age_range,
                                "estilo": estilo,
                                "distancia_m": distancia,
                                "curso": curso,
                                "tiempo_s": float(t_seg),
                            }
                        )
                        STATS["extracted"] += 1
                        if genero == "M":
                            STATS["male"] += 1
                        elif genero == "F":
                            STATS["female"] += 1

    return data_to_insert

# ----------------------------
# Main
# ----------------------------
def ejecutar_cazador() -> None:
    print("ü¶à USMS Masters NQT Scraper v5.1 (SCY + LCM)")

    try:
        html = fetch_html_auto(USMS_NQT_URL)
        sections = iter_sections_with_tables(html)
        if not sections:
            raise RuntimeError("No SCY/LCM Women/Men tables detected (page structure changed?)")

        all_rows: List[Dict] = []
        for info, tbl in sections:
            rows = parse_table_to_rows(info, tbl)
            if not rows:
                continue
            all_rows.extend(rows)

            STATS["courses"][info.course] = STATS["courses"].get(info.course, 0) + len(rows)
            STATS["seasons"][info.season_year] = STATS["seasons"].get(info.season_year, 0) + len(rows)
            for x in rows:
                if x["genero"] == "M":
                    STATS["male"] += 1
                elif x["genero"] == "F":
                    STATS["female"] += 1
            STATS["extracted"] += len(rows)

        if not all_rows:
            raise RuntimeError("Parsed 0 rows from USMS NQT page")

        keys = sorted({(x["season_year"], x["curso"]) for x in all_rows})
        print(f"   üîë Refresh keys: {keys}")
        for season_year, curso in keys:
            delete_existing_for(season_year, curso)
            subset = [x for x in all_rows if x["season_year"] == season_year and x["curso"] == curso]
            insert_batches(subset, batch_size=500)

        STATS["inserted"] = len(all_rows)
        log_final = (
            f"[USMS_MASTERS_NQT] Extracted={STATS['extracted']} Inserted={STATS['inserted']} "
            f"Male={STATS['male']} Female={STATS['female']} "
            f"Courses={STATS['courses']} Seasons={STATS['seasons']}"
        )
        print("\n" + log_final)
        enviar_reporte_email(log_final, "SUCCESS")
        return

    except Exception as e:
        print(f"‚ö†Ô∏è  HTML scrape failed: {e}")
        STATS["errors"] += 1

    # PDF fallback (only if configured)
    try:
        fallback_rows: List[Dict] = []
        if PDF_URL_SCY:
            rr = requests.get(PDF_URL_SCY, headers=REQUEST_HEADERS, timeout=30)
            rr.raise_for_status()
            y = re.search(r"(20\d{2})", PDF_URL_SCY)
            season_year = y.group(1) if y else str(datetime.datetime.now().year)
            fallback_rows.extend(procesar_tablas_pdf(rr.content, "SCY", season_year))
        if PDF_URL_LCM:
            rr = requests.get(PDF_URL_LCM, headers=REQUEST_HEADERS, timeout=30)
            rr.raise_for_status()
            y = re.search(r"(20\d{2})", PDF_URL_LCM)
            season_year = y.group(1) if y else str(datetime.datetime.now().year)
            fallback_rows.extend(procesar_tablas_pdf(rr.content, "LCM", season_year))

        if not fallback_rows:
            raise RuntimeError("No PDF fallback URLs configured or 0 rows extracted")

        keys = sorted({(x["season_year"], x["curso"]) for x in fallback_rows})
        print(f"   üîë Refresh keys (PDF): {keys}")
        for season_year, curso in keys:
            delete_existing_for(season_year, curso)
            subset = [x for x in fallback_rows if x["season_year"] == season_year and x["curso"] == curso]
            insert_batches(subset, batch_size=500)

        log_final = f"[USMS_MASTERS_NQT_PDF] Inserted={len(fallback_rows)} keys={keys}"
        print("\n" + log_final)
        enviar_reporte_email(log_final, "SUCCESS")
        return

    except Exception as e:
        STATS["errors"] += 1
        msg = f"FAILURE: {e}"
        print(msg)
        enviar_reporte_email(msg, "FAILURE")


if __name__ == "__main__":
    ejecutar_cazador()
